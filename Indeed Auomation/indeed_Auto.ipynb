{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(r\"C:\\Users\\Asif-LFD\\AppData\\Local\\Google\\Chrome\\User Data\\Profile 1\")\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options=options)\n",
    "# from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pk.indeed.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input keyword (input variable may be 1. job title 2. where) \n",
    "1. clear() kaam nahi kar rha dyk lyna bad me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_key_word_job = \"Data Engineer\"\n",
    "var_key_word_where = \"karachi\"\n",
    "var_btn_keyword = driver.find_element(\"xpath\",\"/html/body/div/div[1]/div/span/div[3]/div[1]/div/div/div/div/form/div/div[1]/div/div[1]/div/div[2]/input\")\n",
    "var_btn_keyword.clear()\n",
    "var_btn_keyword.send_keys(var_key_word_job)\n",
    "time.sleep(2)\n",
    "\n",
    "var_btn_keyword = driver.find_element(\"xpath\",\"/html/body/div/div[1]/div/span/div[3]/div[1]/div/div/div/div/form/div/div[2]/div/div[1]/div/div[2]/input\")\n",
    "var_btn_keyword.clear()\n",
    "var_btn_keyword.send_keys(var_key_word_where)\n",
    "time.sleep(2)\n",
    "\n",
    "# click find jon button\n",
    "driver.find_element(\"xpath\",\"/html/body/div/div[1]/div/span/div[3]/div[1]/div/div/div/div/form/button\").click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \"+ driver.find_element(\"xpath\",\"/html/body/main/div/div[1]/div/div/div[5]/div[1]/div[4]/div/div/div[2]/span[1]\").text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# left side me companies ki details jo ati list wise os ka code\n",
    "\n",
    "1. div (id = 'mosaic-provider-jobcards') k andar \"ul\" ka tag id\n",
    "2. ul k andar \"li\" ka tag {har li me indivdual company ka card}\n",
    "3. li k andar div class \"job_seen_beacon\" os k andar sara data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cards_company_data = driver.find_element(\"xpath\",\"//div[@id='mosaic-provider-jobcards']\").find_element(\"xpath\",\"ul/li/div\").find_elements(\"xpath\",\"//div[@class='slider_container css-8xisqv eu4oa1w0']\")\n",
    "len(var_cards_company_data)\n",
    "\n",
    "print(len(var_cards_company_data))\n",
    "\n",
    "# saving all data into csv\n",
    "var_csv_cols = ['job_title','company_name',\t'location',\t'salary_range',\t'job_type',\t'description',\t'job_link',\t'indeedApply',\"hiringMultipleCandidates\",\"posted_date\"]\n",
    "\n",
    "\n",
    "# makinf df having columns of var_csv_cols\n",
    "df = pd.DataFrame(columns=var_csv_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(var_cards_company_data)):\n",
    "# for i in range(6,7):\n",
    "\n",
    "\n",
    "    print(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # clicking on each card company name\n",
    "    var_cards_company_data[i].click()\n",
    "    time.sleep(5)\n",
    "    # temp\n",
    "    # soup = BeautifulSoup(var_cards_company_data[10].get_attribute('innerHTML'), 'html.parser')\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    try:\n",
    "        # find salary\n",
    "        salary_range=var_cards_company_data[i].find_element(\"xpath\",\"//table[@class='jobCard_mainContent big6_visualChanges']\").find_element(\"xpath\",\"//tr\").find_element(\"xpath\",\"td\").find_element(\"xpath\",\"div[@class='heading6 tapItem-gutter metadataContainer noJEMChips salaryOnly']\").find_element(\"xpath\",\"div[@class='metadata salary-snippet-container']\").text\n",
    "    except:\n",
    "        salary_range = \"undefine\"\n",
    "    salary_range\n",
    "\n",
    "    # easyapplu and other tags\n",
    "    soup = BeautifulSoup(var_cards_company_data[i].find_element(\"xpath\",\"//tr[@class = 'jobCardShelf']\").get_attribute('innerHTML'), 'html.parser')\n",
    "    # print(soup.prettify())\n",
    "    html = str(soup.prettify())\n",
    "    regex = r'td class=\"shelfItem ([^\"]*)\">'\n",
    "    matches = re.findall(regex, html)\n",
    "    next_words = [match.strip() for match in matches]\n",
    "    # print(next_words)\n",
    "\n",
    "    # posted date\n",
    "    posted_date = var_cards_company_data[i].find_element(\"xpath\",\"//table[@class = 'jobCardShelfContainer big6_visualChanges']\").find_element(\"xpath\",\"//tr[@class = 'underShelfFooter']\").find_element(\"xpath\",\"//span[@class='date']\").text.split(\"\\n\")[-1]\n",
    "    posted_date\n",
    "\n",
    "    #  right portion\n",
    "    # page ka right side\n",
    "    var_right_side = driver.find_element(\"xpath\",\"//div[@class = 'jobsearch-JobComponent icl-u-xs-mt--sm jobsearch-JobComponent--embedded css-axjo09 eu4oa1w0']\")\n",
    "\n",
    "    #ab yaha sy user interction(kisi button py click karna etc etc) kahtam hun gaya just data scrape karna han so we need to use bs4. \n",
    "    # bs4\n",
    "    soup = BeautifulSoup(var_right_side.get_attribute('innerHTML'), 'html.parser')\n",
    "    # print(soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "    ### right up portion (jis me job title,comapny name, location and apply karny ka button han)\n",
    "    # print(soup.find('div',class_=\"jobsearch-InfoHeaderContainer\").prettify())\n",
    "\n",
    "    # job title\n",
    "    job_title = soup.find('h2',class_='icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title is-embedded').text.replace(\" - job post\",\"\")\n",
    "    # job_title\n",
    "\n",
    "    # comapny name\n",
    "    company_name = soup.find(\"div\",class_=\"css-1h46us2 eu4oa1w0\").text #comapny name\n",
    "    # company_name\n",
    "\n",
    "    # location\n",
    "    location = soup.find(\"div\",class_=\"css-6z8o9s eu4oa1w0\").text #location\n",
    "    location\n",
    "\n",
    "    # easyapply\n",
    "    if (soup.find(\"span\",class_='jobsearch-IndeedApplyButton-newDesign css-1hjxf1u eu4oa1w0').text == 'Apply now'):\n",
    "        indeedApply = 1\n",
    "    else:\n",
    "        indeedApply = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # nechy waly code me 2 divs aa rahy. may be contry ka hun. lakin abhi pata nahi q future me kaam aa skta\n",
    "    # soup.find_all(\"div\",class_=\"css-6z8o9s eu4oa1w0\")\n",
    "\n",
    "    ### right down portion (jis me job description wagera han)\n",
    "    var_soup_job_des = soup.find(\"div\",class_='jobsearch-embeddedBody css-1omm75o eu4oa1w0')\n",
    "    var_soup_job_des\n",
    "\n",
    "    try:\n",
    "        # job type\n",
    "        job_type = var_soup_job_des.find_all(\"div\",class_=\"css-tvvxwd ecydgvn1\")\n",
    "        # .text\n",
    "        for divs in job_type:\n",
    "            if divs.text() == \"Job Type\":\n",
    "                job_type = divs.find(\"div\",class_=\"css-1t5f0fr ecydgvn0\").text\n",
    "                break\n",
    "        # job_type\n",
    "\n",
    "\n",
    "\n",
    "    except:\n",
    "        job_type = \"\"\n",
    "\n",
    "\n",
    "    # jobDescriptionText\n",
    "    description = var_soup_job_des.find(\"div\",id=\"jobDescriptionText\").text\n",
    "\n",
    "    # # save in text file\n",
    "    # with open(\"extracted_data_\"+str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))+\".txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    #     f.write(var_soup_job_des.find(\"div\",id=\"jobDescriptionText\").text)\n",
    "    #     f.close()\n",
    "    # description\n",
    "\n",
    "    # getting link\n",
    "    job_link = driver.current_url\n",
    "    job_link\n",
    "\n",
    "    # making an empty dictionary on above listt\n",
    "    var_dict = {i:'' for i in var_csv_cols}\n",
    "    var_dict\n",
    "\n",
    "    # # var_dict[i]\n",
    "    # next_words\n",
    "    # # if next_words have any of var_dict keys than put 1 of each keys values in var_dic\n",
    "    # for i in next_words:\n",
    "    #     if i in var_dict.keys():\n",
    "    #         var_dict[i] = 1\n",
    "    #     else:\n",
    "    #         var_dict[i] = 0\n",
    "\n",
    "    # var_dict\n",
    "    # var_dict.keys()\n",
    "    # description\n",
    "    # var_dict[\"description\"] = description\n",
    "    # var_dict\n",
    "\n",
    "\n",
    "    var_dict[\"job_title\"] =job_title\n",
    "    var_dict[\"company_name\"] =company_name\n",
    "    var_dict[\"location\"] =location\n",
    "    var_dict[\"salary_range\"] =salary_range\n",
    "    var_dict[\"job_type\"] =job_type\n",
    "    var_dict[\"description\"] = description\n",
    "    var_dict[\"job_link\"] =job_link\n",
    "    var_dict[\"posted_date\"] =posted_date\n",
    "\n",
    "\n",
    "    # print(var_dict)\n",
    "\n",
    "    # adding var_dict to df\n",
    "    df = df.append(var_dict,ignore_index=True)\n",
    "    # df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_soup_job_des.find_all(\"div\",class_=\"css-fhkva6 eu4oa1w0\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(var_soup_job_des.find_all(\"div\",class_=\"css-fhkva6 eu4oa1w0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     try:\n",
    "job_type = \"\"\n",
    "\n",
    "job_types = var_soup_job_des.find_all(\"div\",class_=\"css-fhkva6 eu4oa1w0\")\n",
    "# var_soup_job_des.find_all(\"div\",class_=\"css-tvvxwd ecydgvn1\")\n",
    "\n",
    "for divs in range(len(job_types)):\n",
    "        # print(divs.text)\n",
    "\n",
    "        if var_soup_job_des.find_all(\"div\",class_=\"css-fhkva6 eu4oa1w0\")[divs].text == \"Job Type\":\n",
    "                job_type = var_soup_job_des.find_all(\"div\",class_=\"css-tvvxwd ecydgvn1\")[divs].text\n",
    "                break\n",
    "job_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving file with current time with seconds\n",
    "df.to_csv(\"extracted_data_\"+str(datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))+\".csv\",index=False)\n",
    "# df.to_csv(\"extracted_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left side div class = \"css-tvvxwd ecydgvn1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def keyword_matching(resume, job_description):\n",
    "#     resume_keywords = set(re.findall(r'\\b\\w+\\b', resume.lower()))\n",
    "#     job_keywords = set(re.findall(r'\\b\\w+\\b', job_description.lower()))\n",
    "#     matched_keywords = resume_keywords.intersection(job_keywords)\n",
    "#     return matched_keywords\n",
    "\n",
    "# # Example usage\n",
    "# resume_text = '''\n",
    "\n",
    "# EHTISHAM ALI\n",
    "# 03412373830 | etsali232@gmail.com | linkedin.com/in/ali-ehtisham/ | github.com/ehtish\n",
    "# Objective\n",
    "# Skilled web developer with expertise in HTML, CSS, JavaScript, Bootstrap, and React. Passionate about creating visually stunning and user-friendly web experiences. Seeking a challenging role to leverage MERN stack proficiency and drive impactful solutions that exceed client expectations.\n",
    "# Experience\n",
    "# CONTOUR SOFTWARE\n",
    "# MERN stack trainee\n",
    "# - During the comprehensive MERN (MongoDB, Express, React, Node) stack development training, hands-on experience was gained in building and deploying responsive web applications.\n",
    "# - Worked under the supervision of industry experts to provide solutions to real-world problems.\n",
    "# - Developed proficiency in front-end development, creating dynamic user interfaces using React.\n",
    "# - Furthermore, valuable knowledge and extensive practical experience were diligently acquired in API building and proficiently handling hosting operations.\n",
    "# Projects\n",
    "# 1. Exercise Tracker (Final Project at CONTOUR)\n",
    "# - The Exercise Tracker project utilizes the MERN stack, including MongoDB, Express, React, and Node.js, along with Bootstrap and Material UI. It offers a visually appealing, user-friendly experience with a responsive design, catering to various devices. Secure login is implemented using JWT tokens for seamless activity management.\n",
    "# 2. Managee\n",
    "# - Efficiently developing the front-end of the software team management website, the project showcased proficient utilization of HTML, CSS, JavaScript, jQuery, and the feature-rich JavaScript library, Owl Carousel. With a strong emphasis on responsive design, the website ensures a seamless user experience and intuitive features.\n",
    "# - Live demo: https://ehtish.github.io/managee/\n",
    "# 3. Mr. Mashwara\n",
    "# - Developed a chrome extension project utilizing the advice API with skillful implementation of HTML5, CSS3, JavaScript, and JSON.\n",
    "# - Live demo: https://github.com/Ehtish/Mr.Mashwara\n",
    "# 4. Fylo\n",
    "# - Skillfully created Fylo, a file securing website, utilizing HTML5 and CSS3 with a special feature of dynamically responsive font size functionality, showcasing technical expertise and innovation.\n",
    "# - Live demo: https://ehtish.github.io/fylo-dark/\n",
    "# 5. Portfolio\n",
    "# - Recently collaborated on a dynamic portfolio project that demonstrated proficiency in HTML5, CSS3, Bootstrap 4, and JavaScript. Impressively showcased front-end skills through custom styling, responsive design, and engaging interactive features.\n",
    "# - Live demo: https://ehtish.github.io/asif-portfolio/\n",
    "# - Note: Explore more projects and contributions on my GitHub profile: https://github.com/Ehtish\n",
    "# Academics\n",
    "# BSCS\n",
    "# SINDH MADRASATUL ISLAM UNVERSITY\n",
    "# Jan 2016 -2020\n",
    "# Skills\n",
    "# - HTML5, CSS3, Javascript, Bootstrap, React, Node, Express, and MongoDB.\n",
    "# Soft Skills\n",
    "# - Communication, Problem Solving, Adaptability, Teamwork and Time Management.\n",
    "# Platforms - Git, GitHub, Netlify and MySQL.\n",
    "# Beyond Academics - Reading Books and Gaming.\n",
    "# '''\n",
    "\n",
    "# job_description_text = '''\n",
    "# A Frontend Developer is responsible for creating and implementing the visual and interactive elements of a website or application. They work closely with designers, back-end developers, and stakeholders to ensure that the user experience is seamless and intuitive. Frontend Developers use a combination of programming languages and tools to bring design concepts to life, and must be able to balance technical skills with creativity and attention to detail.\n",
    "\n",
    "# Key Responsibilities:\n",
    "# - Collaborate with designers, back-end developers, and stakeholders to develop and implement user-friendly interfaces and interactive features\n",
    "# - Write clean, efficient, and well-documented code using HTML, CSS, and JavaScript\n",
    "# - Hands-on experience with JS libraries and frameworks like ReactJS, VueJS, Angular\n",
    "# - Should be able to connect APIs with frontend shared by backend team\n",
    "# - Optimize web pages for maximum speed and scalability\n",
    "# - Ensure cross-browser and cross-device compatibility\n",
    "# - Identify and troubleshoot issues related to performance, user experience, and accessibility\n",
    "# - Stay up-to-date with emerging trends and technologies in web development\n",
    "# - Conduct usability testing and gather feedback from users to improve website or application design\n",
    "# - Communicate technical information to non-technical stakeholders\n",
    "\n",
    "# Qualifications:\n",
    "# - Bachelor's degree in Computer Science, Software Engineering, or related field\n",
    "# - Proven experience as a Frontend Developer or similar role\n",
    "# - Proficiency in HTML, CSS, JavaScript, and related frameworks such as React, Angular, or Vue.js\n",
    "# - Experience with responsive design, mobile-first development, and cross-browser compatibility\n",
    "# - Knowledge of user experience (UX) design principles and methodologies\n",
    "# - Familiarity with version control tools such as Git\n",
    "# - Strong problem-solving and analytical skills\n",
    "# - Excellent communication and teamwork abilities\n",
    "\n",
    "# Preferred Qualifications:\n",
    "# - Experience with web performance optimization techniques\n",
    "# - Familiarity with server-side scripting languages such as PHP or Node.js\n",
    "# - Knowledge of search engine optimization (SEO) best practices\n",
    "# - Experience with web accessibility standards and guidelines (WCAG)\n",
    "# - Strong portfolio showcasing previous web development projects\n",
    "\n",
    "# Job Type: Full-tim\n",
    "# '''\n",
    "\n",
    "# matched_keywords = keyword_matching(resume_text, job_description_text)\n",
    "\n",
    "# print(\"Matched Keywords:\", matched_keywords)\n",
    "# print(\"Number of Matches:\", len(matched_keywords))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
